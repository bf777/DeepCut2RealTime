{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "deepcut2realtime.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "38baca47ce314463bff519ae80943265": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DropdownModel",
     "state": {
      "_options_labels": [
       "full_human",
       "full_cat",
       "full_dog",
       "primate_face",
       "mouse_pupil_vclose"
      ],
      "_view_name": "DropdownView",
      "style": "IPY_MODEL_682c9f76270e48e4845206fa5719c4d7",
      "_dom_classes": [],
      "description": "Choose a DLC ModelZoo model!",
      "_model_name": "DropdownModel",
      "index": 0,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "disabled": false,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_ee1f1171373d430b9a0b76bd58d835f9"
     }
    },
    "682c9f76270e48e4845206fa5719c4d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "ee1f1171373d430b9a0b76bd58d835f9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "OoRBrhOHmW7S",
    "colab_type": "text"
   },
   "source": [
    "## DeepCut2RealTime\n",
    "This is a notebook-based implementation of our real-time behaviour estimation and\n",
    "reinforcement system. It is adapted from the code that was used to carry out the behavioural experiments outlined\n",
    "in our _eNeuro_ paper: [Forys, Xiao, Gupta, and Murphy (2020)](https://doi.org/10.1523/ENEURO.0096-20.2020),\n",
    "and builds upon the code outlined in our _bioRxiV_ preprint: [Forys, Xiao, Gupta, Boyd, and Murphy (2018)](https://doi.org/10.1101/482349). It's based on the [DeepLabCut toolbox](http://www.mousemotorlab.org/deeplabcut) created by [Mathis et al. (2018)](https://doi.org/10.1038/s41593-018-0209-y).\n",
    "\n",
    "This notebook will allow you to track user-defined behaviours in real time from your webcam.\n",
    "In order to do this, we'll:\n",
    "1. Set up DeepLabCut and our custom real-time tracking code\n",
    "2. Select or upload a DeepLabCut model to define the behaviour to be tracked\n",
    "3. Define our criterion behaviour\n",
    "4. Start tracking the behaviour!\n",
    "\n",
    "NOTE: for best results, ensure that you are using a GPU by going to the menu above and clicking `Runtime > Change runtime type`, then selecting `GPU` from the dropdown menu. \n",
    "\n",
    "If you're viewing this page in GitHub, click the following button to run this code in Google Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bf777/deepcut2realtime/blob/master/notebooks/deepcut2realtime.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zwxu1oIvmW7U",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# To begin, let's install some necessary modules (this step\n",
    "# may take a few minutes).\n",
    "!pip install deeplabcut pandas scikit-image Pillow opencv-python\n",
    "%reload_ext numpy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DY9sVgQHmW7Z",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Use TensorFlow 1.x:\n",
    "%tensorflow_version 1.x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dycUkTVQmW7d",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#GUIs don't work on the cloud, so we supress them:\n",
    "import os\n",
    "os.environ[\"DLClight\"]=\"True\"\n",
    "\n",
    "# stifle tensorflow warnings, like we get it already.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Now, let's import the modules we need.\n",
    "import os.path\n",
    "from deeplabcut.pose_estimation_tensorflow.nnet import predict\n",
    "from deeplabcut.pose_estimation_tensorflow.config import load_config\n",
    "from deeplabcut.utils import auxiliaryfunctions\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import skimage\n",
    "from skimage.util import img_as_ubyte\n",
    "import logging\n",
    "\n",
    "# Real-time tracking dependencies\n",
    "import _thread\n",
    "from copy import deepcopy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z73RWmZ93YV6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Mount google drive. If you save your custom DeepLabCut model in your Google Drive,\n",
    "# you'll be able to access it here. Additionally, this will let you directly save\n",
    "# the outputs of your model to Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pD5Bkw3hmW7h",
    "colab_type": "text"
   },
   "source": [
    "Now, we'll define `analyze_stream`, the top-level function for running the real-time tracking\n",
    "(adapted from DeepLabCut):\n",
    "\n",
    "We will define a criterion movement for the body part that we're analyzing. If the body part moves in a way that\n",
    "satisfies this criterion between two frames, we can record the behaviour as having met criterion (demonstrating how\n",
    "feedback would be given based on this user-defined movement)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cJ5bZoMOmW7h",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# THRESHOLDS FOR Y MOVEMENT\n",
    "# min y movement on left paw to be counted as a trigger = 5 px\n",
    "y_left = 5 #@param {type: \"number\"}\n",
    "# max y movement on right paw to be counted as a trigger = 10 px\n",
    "y_right = 10 #@param {type: \"number\"}\n",
    "# Max y movement overall: 100 px\n",
    "y_upper_lim = 100 #@param {type: \"number\"}\n",
    "# Refractory time (s): time after last trigger during which no LED flash can be triggered/reward given\n",
    "refractory_time = 0.3 #@param {type: \"number\"}"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "b9rR55XWmW7k",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def analyze_stream(config, destfolder, y_left, y_right, y_upper_lim, refractory_time,\n",
    "                   shuffle=1, trainingsetindex=0, gputouse=0, save_as_csv=False, save_frames=True,\n",
    "                   cropping=None, baseline=True, name=\"default_animal\", camtype=\"colab\"):\n",
    "    if 'TF_CUDNN_USE_AUTOTUNE' in os.environ:\n",
    "        del os.environ['TF_CUDNN_USE_AUTOTUNE']  # was potentially set during training\n",
    "\n",
    "    if gputouse is not None:  # gpu selection\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    start_path = os.getcwd()  # record cwd to return to this directory in the end\n",
    "\n",
    "    cfg = auxiliaryfunctions.read_config(config)\n",
    "\n",
    "    if cropping is not None:\n",
    "        cfg['cropping'] = True\n",
    "        cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2'] = cropping\n",
    "        print(\"Overwriting cropping parameters:\", cropping)\n",
    "        print(\"These are used for all videos, but won't be save to the cfg file.\")\n",
    "\n",
    "    trainFraction = cfg['TrainingFraction'][trainingsetindex]\n",
    "\n",
    "    modelfolder = os.path.join(cfg[\"project_path\"], str(auxiliaryfunctions.GetModelFolder(trainFraction, shuffle, cfg)))\n",
    "    path_test_config = Path(modelfolder) / 'test' / 'pose_cfg.yaml'\n",
    "    try:\n",
    "        dlc_cfg = load_config(str(path_test_config))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            \"It seems the model for shuffle %s and trainFraction %s does not exist.\" % (shuffle, trainFraction))\n",
    "\n",
    "    # Check which snapshots are available and sort them by # iterations\n",
    "    try:\n",
    "        Snapshots = np.array(\n",
    "            [fn.split('.')[0] for fn in os.listdir(os.path.join(modelfolder, 'train')) if \"index\" in fn])\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            \"Snapshots not found! It seems the dataset for shuffle %s has not been trained/does not exist.\\n Please train it before using it to analyze videos.\\n Use the function 'train_network' to train the network for shuffle %s.\" % (\n",
    "            shuffle, shuffle))\n",
    "\n",
    "    if cfg['snapshotindex'] == 'all':\n",
    "        print(\n",
    "            \"Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!\")\n",
    "        snapshotindex = -1\n",
    "    else:\n",
    "        snapshotindex = cfg['snapshotindex']\n",
    "\n",
    "    increasing_indices = np.argsort([int(m.split('-')[1]) for m in Snapshots])\n",
    "    Snapshots = Snapshots[increasing_indices]\n",
    "\n",
    "    print(\"Using %s\" % Snapshots[snapshotindex], \"for model\", modelfolder)\n",
    "\n",
    "    ##################################################\n",
    "    # Load and setup CNN part detector\n",
    "    ##################################################\n",
    "\n",
    "    # Check if data already was generated:\n",
    "    dlc_cfg['init_weights'] = os.path.join(modelfolder, 'train', Snapshots[snapshotindex])\n",
    "    trainingsiterations = (dlc_cfg['init_weights'].split(os.sep)[-1]).split('-')[-1]\n",
    "\n",
    "    # update batchsize (based on parameters in config.yaml)\n",
    "    dlc_cfg['batch_size'] = cfg['batch_size']\n",
    "    # Name for scorer:\n",
    "    DLCscorer = auxiliaryfunctions.GetScorerName(cfg, shuffle, trainFraction, trainingsiterations=trainingsiterations)\n",
    "\n",
    "    sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)\n",
    "    pdindex = pd.MultiIndex.from_product([[DLCscorer], dlc_cfg['all_joints_names'], ['x', 'y', 'likelihood']],\n",
    "                                         names=['scorer', 'bodyparts', 'coords'])\n",
    "\n",
    "    if gputouse is not None:  # gpu selectinon\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gputouse)\n",
    "\n",
    "    ##################################################\n",
    "    # Set up data buffer and global variables to be used in threads\n",
    "    ##################################################\n",
    "    global PredicteData\n",
    "    PredicteData = np.zeros((50000, 3 * len(dlc_cfg['all_joints_names'])))\n",
    "    global led_arr\n",
    "    led_arr = np.zeros((50000, 7))\n",
    "    global x_range\n",
    "    global y_range\n",
    "    global acc_range\n",
    "    x_range = list(range(0, (3 * len(dlc_cfg['all_joints_names'])), 3))\n",
    "    y_range = list(range(1, (3 * len(dlc_cfg['all_joints_names'])), 3))\n",
    "    acc_range = list(range(2, (3 * len(dlc_cfg['all_joints_names'])), 3))\n",
    "    global colors\n",
    "    colors = [(0, 0, 255), (0, 165, 255), (0, 255, 255), (0, 255, 0), (255, 0, 0), (240, 32, 160), (0, 0, 255),\n",
    "              (0, 165, 255)]\n",
    "    global empty_count\n",
    "    empty_count = 0\n",
    "    global threshold_count\n",
    "    AnalyzeStream(DLCscorer, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, pdindex, save_as_csv, save_frames,\n",
    "                  destfolder, name, baseline, camtype, y_left, y_right, y_upper_lim, refractory_time)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "UuCRp6J7mW7n",
    "colab_type": "text"
   },
   "source": [
    "The next function we'll define is `GetPoseS`, our modified version of\n",
    "the DeepLabCut function of the same name that takes a batch of two\n",
    "frames from the webcam as an input and sets up pose estimation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SodIhEjdmW7o",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from google.colab import output\n",
    "\n",
    "def GetPoseS(cfg, dlc_cfg, sess, inputs, outputs, cap, w, h, nframes, save_frames, destfolder, baseline, camtype,\n",
    "             y_left, y_right, y_upper_lim, refractory_time):\n",
    "    \"\"\" Non batch wise pose estimation for video cap.\"\"\"\n",
    "    # Prepare data arrays\n",
    "    global lastFrameWasMoved\n",
    "    global thisFrameWasMoved\n",
    "    global threshold_count\n",
    "    lastFrameWasMoved = False\n",
    "    thisFrameWasMoved = False\n",
    "    x_arr = []\n",
    "    y_arr = []\n",
    "    if cfg['cropping']:\n",
    "        print(\n",
    "            \"Cropping based on the x1 = %s x2 = %s y1 = %s y2 = %s. You can adjust the cropping coordinates in the config.yaml file.\" % (\n",
    "            cfg['x1'], cfg['x2'], cfg['y1'], cfg['y2']))\n",
    "        if w > 0 and h > 0:\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('Please check the order of cropping parameter!')\n",
    "    LED = ''\n",
    "    start = time.time()\n",
    "    counter = 0\n",
    "    threshold_count = 0\n",
    "    frame_arr = []\n",
    "    try:\n",
    "        while cap:\n",
    "            if camtype == 'cv2':\n",
    "                ret, frame = cap.read()\n",
    "            elif camtype == 'colab':\n",
    "                frame = take_photo()\n",
    "            # if ret:\n",
    "            if len(frame) > 0:\n",
    "                # print(\"FRAME RECEIVED\")\n",
    "                frame = skimage.color.gray2rgb(frame)\n",
    "                if cfg['cropping']:\n",
    "                    frame = img_as_ubyte(frame[cfg['y1']:cfg['y2'], cfg['x1']:cfg['x2']])\n",
    "                else:\n",
    "                    frame = img_as_ubyte(frame)\n",
    "                frame_arr.append(frame)\n",
    "                frame_time = time.time()\n",
    "                led_arr[counter, 0] = frame_time\n",
    "                led_arr[counter + 1, 0] = frame_time\n",
    "                if (time.time() - start) > 0:\n",
    "                    print(\"Current FPS: {} fps, Time elapsed: {} s, Number of frames so far: {}\".format(\n",
    "                        round(counter / (time.time() - start), 2),\n",
    "                        round(time.time() - start, 2), counter))\n",
    "                if len(frame_arr) == 2:\n",
    "                    # This thread carries out pose estimation on each batch of two frames that arrives from the camera.\n",
    "                    _thread.start_new_thread(frame_process, (frame_arr, dlc_cfg, sess, inputs, outputs, counter,\n",
    "                                                             save_frames, destfolder, LED, x_arr, y_arr, frame_time,\n",
    "                                                             start, baseline, y_left, y_right, y_upper_lim,\n",
    "                                                             refractory_time))\n",
    "                    frame_arr = []\n",
    "                    counter += 2\n",
    "            x_arr = []\n",
    "            y_arr = []\n",
    "            x_first = 0\n",
    "            y_first = 0\n",
    "            threshold = 0\n",
    "            # Run each trial for 130 seconds\n",
    "            if time.time() - start >= 30:\n",
    "                nframes = counter\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        _thread.exit()\n",
    "        print(\"Finished.\")\n",
    "        cap.release()\n",
    "        exit()\n",
    "    # LED.close()\n",
    "    return nframes"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "sEDTyFnNmW7r",
    "colab_type": "text"
   },
   "source": [
    "Now, we'll define `AnalyzeStream`, which manages the overall\n",
    "operation of the real-time tracking trial and which handles the camera\n",
    "connection and data saving."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "qJXFVeCpmW7s",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def AnalyzeStream(DLCscorer, trainFraction, cfg, dlc_cfg, sess, inputs, outputs, pdindex, save_as_csv, save_frames,\n",
    "                  destfolder, name, baseline, camtype, y_left, y_right, y_upper_lim, refractory_time):\n",
    "    \"\"\"Sets up camera connection for pose estimation, and handles data output.\"\"\"\n",
    "    # Setup camera connection\n",
    "    # if camtype == 'cv2':\n",
    "    #     cam = cv2.VideoCapture(0)\n",
    "    #     print(\"CV2-compatible camera connected!\")\n",
    "    #     size = (int(cam.get(3)), int(cam.get(4)))\n",
    "    if camtype == 'colab':\n",
    "        cam = 'colab'\n",
    "\n",
    "    print(\"Starting to analyze stream\")\n",
    "    # Accept a single connection and make a file-like object out of it\n",
    "    cap = cam\n",
    "    dataname = os.path.join(destfolder, '{}_{}.h5'.format(DLCscorer, name))\n",
    "    dataname_led = os.path.join(destfolder, '{}_{}_LED.h5'.format(DLCscorer, name))\n",
    "    led_data_cols = ['FrameTime', 'MovementDiffLeft', 'MovementDiffRight', 'ThresholdTime', 'Delay', 'FlashTime',\n",
    "                     'WaterTime']\n",
    "    size = (200, 200)\n",
    "    w, h = size\n",
    "    shutter = 1/500\n",
    "    brightness = 68\n",
    "    v_blanking = 982\n",
    "    acc_tolerance = 0.20\n",
    "    missing_count = 0\n",
    "    nframes = 0\n",
    "\n",
    "    print(\"Starting to extract posture\")\n",
    "    start = time.time()\n",
    "    nframes = GetPoseS(cfg, dlc_cfg, sess, inputs, outputs, cap, w, h, nframes, save_frames, destfolder, baseline,\n",
    "                       camtype, y_left, y_right, y_upper_lim, refractory_time)\n",
    "\n",
    "    # stop the timer and display FPS information\n",
    "    stop = time.time()\n",
    "    fps = nframes / (stop - start)\n",
    "    print(\"\\n\")\n",
    "    print(\"[INFO] elasped time: {:.2f}\".format(stop - start))\n",
    "    print(\"[INFO] approx. FPS: {:.2f}\".format(fps))\n",
    "\n",
    "    # If there's rows with blank data at the end of the trial, record this data as missing/dropped frames\n",
    "    for row in PredicteData[int(np.around(fps * 10)):nframes, :]:\n",
    "        if 0 in row:\n",
    "            missing_count += 1\n",
    "\n",
    "    time.sleep(10)\n",
    "    avg_array = led_arr[:, 4]\n",
    "    avg_delay = avg_array[avg_array != 0].mean()\n",
    "    sd_delay = np.std(avg_array[avg_array != 0])\n",
    "    avg_acc = PredicteData[:nframes, acc_range].mean()\n",
    "\n",
    "    # Prints out results of trial\n",
    "    print(\"Empty values: {}, {} per second\".format(str(missing_count), str(missing_count / (stop - start))))\n",
    "    print(\"Adjusted frame rate: {}\".format(str((nframes - missing_count) / (stop - start))))\n",
    "    print(\"Average delay: {} s\".format(str(avg_delay)))\n",
    "    print(\"Standard dev. of delay: {} s\".format((str(sd_delay))))\n",
    "    print(\"Average tracking accuracy: {}\".format((str(avg_acc))))\n",
    "\n",
    "    # Save metadata with trial information and camera information\n",
    "    dictionary = {\n",
    "        \"name\": name,\n",
    "        \"start\": start,\n",
    "        \"stop\": stop,\n",
    "        \"run_duration\": stop - start,\n",
    "        \"Scorer\": DLCscorer,\n",
    "        \"DLC-model-config file\": dlc_cfg,\n",
    "        \"fps\": fps,\n",
    "        \"fps_adjusted\": ((nframes - missing_count) / (stop - start)),\n",
    "        \"avg_delay\": avg_delay,\n",
    "        \"sd_delay\": sd_delay,\n",
    "        \"v_blanking\": v_blanking,\n",
    "        \"shutter\": shutter,\n",
    "        \"brightness\": brightness,\n",
    "        \"batch_size\": dlc_cfg[\"batch_size\"],\n",
    "        \"frame_dimensions\": (h, w),\n",
    "        \"nframes\": nframes,\n",
    "        \"acc_tolerance\": acc_tolerance,\n",
    "        \"avg_acc\": avg_acc,\n",
    "        \"iteration (active-learning)\": cfg[\"iteration\"],\n",
    "        \"training set fraction\": trainFraction,\n",
    "        \"cropping\": cfg['cropping'],\n",
    "        \"LED_time\": 0.2,\n",
    "        \"water_time\": 0.15,\n",
    "        \"refractory_period\": 0.3\n",
    "    }\n",
    "    metadata = {'data': dictionary}\n",
    "\n",
    "    print(\"Saving results in {} and {}\".format(dataname, dataname_led))\n",
    "    auxiliaryfunctions.SaveData(PredicteData[:nframes, :], metadata, dataname, pdindex, range(nframes), save_as_csv)\n",
    "    auxiliaryfunctions.SaveData(led_arr[:nframes, :], metadata, dataname_led, led_data_cols, range(nframes),\n",
    "                                save_as_csv)\n",
    "    # cam.release()"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6lgCCDAsmW7w",
    "colab_type": "text"
   },
   "source": [
    "The function `frame_process` handles the pose estimation for each batch\n",
    "of two frames."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eZaJN3blmW7x",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def frame_process(frame_arr, dlc_cfg, sess, inputs, outputs, counter, save_frames, destfolder, LED, x_arr, y_arr,\n",
    "                  frame_time, start, baseline, y_left, y_right, y_upper_lim, refractory_time):\n",
    "    \"\"\"Estimates pose in each frame, and optionally plots the pose on each frame.\"\"\"\n",
    "    # Set up parameters for refractory period (to check whether there was movement on the previous frame).\n",
    "    global thisFrameWasMoved\n",
    "    global lastFrameWasMoved\n",
    "    global threshold_time\n",
    "    global threshold_count\n",
    "    x_avg_left = []\n",
    "    y_avg_left = []\n",
    "    x_avg_right = []\n",
    "    y_avg_right = []\n",
    "    # Run DeepLabCut pose prediction on each batch of two frames\n",
    "    for n, frame in enumerate(frame_arr):\n",
    "        pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)\n",
    "        # if time.time() - start >= 10:\n",
    "        PredicteData[counter + n, :] = pose.flatten()  # NOTE: thereby cfg['all_joints_names'] should be same order as bodyparts!\n",
    "        # Create lists to store average x and y paw positions\n",
    "        for x_val, y_val in zip(x_range, y_range):\n",
    "            x_arr.append(PredicteData[counter + n, :][x_val])\n",
    "            y_arr.append(PredicteData[counter + n, :][y_val])\n",
    "        if n == 0:\n",
    "            add = 0\n",
    "        elif n == 1:\n",
    "            add = 8\n",
    "        x_avg_right.append(np.mean(x_arr[0 + add:3 + add]))\n",
    "        x_avg_left.append(np.mean(x_arr[4 + add:7 + add]))\n",
    "        y_avg_right.append(np.mean(y_arr[0 + add:3 + add]))\n",
    "        y_avg_left.append(np.mean(y_arr[4 + add:7 + add]))\n",
    "        # print(y_avg_left)\n",
    "        # print(y_avg_right)\n",
    "        thisFrameWasMoved = False\n",
    "        # print(\"POSE ESTIMATION LENGTH: {}\".format(n))\n",
    "        # print(\"LENGTH: {}\".format(len(y_avg_left)))\n",
    "        if len(y_avg_left) >= 2:\n",
    "          threshold_calc(counter, start, y_avg_left, y_avg_right, y_upper_lim, \n",
    "                         thisFrameWasMoved, y_left, y_right, refractory_time, \n",
    "                         frame_time, threshold_count, lastFrameWasMoved)\n",
    "        if save_frames:\n",
    "          _thread.start_new_thread(frame_save_func, (frame, x_range, y_range, \n",
    "                                                     x_avg_left, y_avg_left,\n",
    "                                                      x_avg_right, y_avg_right, \n",
    "                                                     destfolder, n, counter))"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rxFEDrpMmW7z",
    "colab_type": "text"
   },
   "source": [
    "Now, we'll define the function used to evaluate whether movement between these two frames was greater than the criterion\n",
    "that we defined. If it is, we'll record that the movement was beyond criterion."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PAW-UH3MmW70",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def threshold_calc(counter, start, y_avg_left, y_avg_right, y_upper_lim, \n",
    "                   thisFrameWasMoved, y_left, y_right, refractory_time, \n",
    "                   frame_time, threshold_count, lastFrameWasMoved):\n",
    "    # We wait for 10.1 s after the start of the trial to start giving behavioural feedback. This is because the latency\n",
    "    # for pose estimation is longer, and the framerate is less stable, in the first 10 s of the trial\n",
    "    # (likely because of Python library setup).\n",
    "    if counter >= 1:\n",
    "        # print(\"Left: {}\".format(y_avg_left))\n",
    "        # print(\"Right: {}\".format(y_avg_right))\n",
    "        trial_start = True\n",
    "        # if not baseline:\n",
    "        #     _thread.start_new_thread(led_task, (LED, trial_start, trial_length - 10)),\n",
    "        led_arr[counter, 1] = abs(y_avg_left[1] - y_avg_left[0])\n",
    "        led_arr[counter, 2] = abs(y_avg_right[1] - y_avg_right[0])\n",
    "\n",
    "        '''\n",
    "        Below is the code used to define the criterion for movement. By default,\n",
    "        it's configured for eight-point tracking in which the first four points\n",
    "        are the right paw and the last four points are the left paw. If the left\n",
    "        paw moves more than a given criterion, the movement will be recorded as\n",
    "        such in the .csv output file labelled \"LED\". In our actual code, this\n",
    "        would be used to trigger an LED flash and water reward (handled by the\n",
    "        commented-out `thresholdFunc` function). Feel free to play around with\n",
    "        this code, along with adjusting the parameters as defined in an earlier\n",
    "        cell.\n",
    "        '''\n",
    "        if (y_left <= abs(y_avg_left[1] - y_avg_left[0]) <= y_upper_lim and\n",
    "                y_right >= abs(y_avg_right[1] - y_avg_right[0])):\n",
    "            # and PredicteData[counter, acc_range].mean() >= 0.20\n",
    "            print(\"CRITERION MOVEMENT: {}\".format(abs(y_avg_left[1] - \n",
    "                                                      y_avg_left[0])))\n",
    "            thisFrameWasMoved = True\n",
    "            if threshold_count == 0:\n",
    "                threshold_time = time.time() - 0.5\n",
    "        if thisFrameWasMoved and not lastFrameWasMoved and abs(time.time() - threshold_time) >= refractory_time:\n",
    "            threshold_time = time.time()\n",
    "            threshold_count += 1\n",
    "            delay = threshold_time - frame_time\n",
    "            led_arr[counter + 1, 3] = threshold_time\n",
    "            led_arr[counter + 1, 4] = delay\n",
    "            ttt = True\n",
    "            # _thread.start_new_thread(thresholdFunc, (LED, baseline, ttt, gpio_light_time, gpio_water_time, counter,\n",
    "            #                                          threshold_time))\n",
    "        lastFrameWasMoved = thisFrameWasMoved"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ba8qGmraEBYW",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Save frames\n",
    "def frame_save_func(frame, x_range, y_range, x_avg_left, y_avg_left, \n",
    "                    x_avg_right, y_avg_right, destfolder, n, counter):\n",
    "    '''Handles plotting estimated paw positions on frames, and saving these frames.'''\n",
    "    tmp = deepcopy(frame)\n",
    "    avgs = False\n",
    "    if avgs:\n",
    "        cv2.circle(tmp, (int(x_avg_right[n]), int(y_avg_right[n])), 6, \n",
    "                   (0, 165, 255), -1)\n",
    "        cv2.circle(tmp, (int(x_avg_left[n]), int(y_avg_left[n])), 6, \n",
    "                   (0, 165, 255), -1)\n",
    "    else:\n",
    "        for x_plt, y_plt, c in zip(x_range, y_range, colors):\n",
    "            cv2.circle(tmp, (int(PredicteData[counter + n, :][x_plt]),\n",
    "                             int(PredicteData[counter + n, :][y_plt])), 2, c, -1)\n",
    "    cv2.imwrite(os.path.join(destfolder, \n",
    "                             'frame{}.png'.format(str(counter + n))), tmp)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0GkgIeTKRDal",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "'''\n",
    "Set folder for data input/output in your Google drive (make sure to\n",
    "create a folder with this name in your Google drive!)\n",
    "'''\n",
    "gDriveParentFolder = 'deepcut2realtime' #@param {type: \"string\"}"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "H-Bi77MdmW72",
    "colab_type": "text"
   },
   "source": [
    "We have now defined all of the code that we need to run the real-time tracking and record data accordingly.\n",
    "Now we need to select the behavioural model to use, and connect to the webcam.\n",
    "\n",
    "To pick a pretrained model from the [DeepLabCut Model Zoo](http://www.mousemotorlab.org/dlc-modelzoo/),\n",
    "run the following two cells. To upload your own model, skip the next two cells and run the one that follows them. The\n",
    "following two cells are from the DeepLabCut developers' Model Zoo [Colab notebook](https://colab.research.google.com/github/AlexEMG/DeepLabCut/blob/master/examples/COLAB_DLC_ModelZoo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Mx_d9fHSmW73",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "38baca47ce314463bff519ae80943265",
      "682c9f76270e48e4845206fa5719c4d7",
      "ee1f1171373d430b9a0b76bd58d835f9"
     ]
    },
    "outputId": "50ba5f6f-8769-4acc-e441-15aa2e24b66f"
   },
   "source": [
    "# Select a DLC ModelZoo model\n",
    "import deeplabcut\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "model_options = deeplabcut.create_project.modelzoo.Modeloptions\n",
    "model_selection = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value=model_options[0],\n",
    "    description=\"Choose a DLC ModelZoo model!\",\n",
    "    disabled=False\n",
    ")\n",
    "display(model_selection)\n",
    "modelzoo = True"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38baca47ce314463bff519ae80943265",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "Dropdown(description='Choose a DLC ModelZoo model!', options=('full_human', 'full_cat', 'full_dog', 'primate_f…"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "E4sGAwUmmW76",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# You can change the name of the project here by editing ProjectFolderName and YourName\n",
    "ProjectFolderName = 'myDLC_modelZoo' #@param {type: \"string\"}\n",
    "YourName = 'teamDLC' #@param {type: \"string\"}\n",
    "model2use = model_selection.value # see \n",
    "# http://www.mousemotorlab.org/dlc-modelzoo for the list! (curently: full_dog,\n",
    "# full_cat, full_human, primate_face)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "fpiOl0XWmW78",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "'''\n",
    "ONLY run this cell if you want to upload your own model! If so, upload your\n",
    "project folder to google drive, then add the name of the folder where you're \n",
    "keeping all of your outputs under `gDriveParentFolder` and your DeepLabCut \n",
    "folder here.\n",
    "'''\n",
    "ManualProjectFolderName = 'latest_model' #@param {type: \"string\"}\n",
    "# Edit the config file's project path to work on Colab.\n",
    "config_path = os.path.join('/content/drive/My Drive/', gDriveParentFolder, \n",
    "                           ManualProjectFolderName, 'config.yaml')\n",
    "config_path_parent = os.path.join('/content/drive/My Drive/', gDriveParentFolder, \n",
    "                           ManualProjectFolderName)\n",
    "edits = {'project_path': config_path_parent}\n",
    "auxiliaryfunctions.edit_config(config_path, edits)\n",
    "modelzoo = False"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mg9HbE0H-y-C",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "config_path"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "o3LSbjesmW7_",
    "colab_type": "text"
   },
   "source": [
    "If we've selected a model from the DeepLabCut Model Zoo, we now need to generate a config folder for the project. Don't\n",
    "run the following code cell if you're using your own model!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "EbbW9vrzmW7_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "print(os.getcwd())\n",
    "'''\n",
    "DeepLabCut expects a video - even though we don't need one for real-time\n",
    "tracking ;) - so we've provided a placeholder video in the git repository that\n",
    "will be used to keep DeepLabCut happy.\n",
    "'''\n",
    "# Fetch a single <1MB file using the raw GitHub URL.\n",
    "!curl --remote-name \\\n",
    "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
    "     --location https://api.github.com/repos/bf777/DeepCut2RealTime/contents/notebooks/test.avi\n",
    "video_path = '.'\n",
    "videotype = '.avi'\n",
    "config_path = deeplabcut.create_pretrained_project(ProjectFolderName, YourName, [video_path],\n",
    "                                                        videotype=videotype, model=model2use, analyzevideo=False,\n",
    "                                                        createlabeledvideo=False, copy_videos=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FRd4EIwgmW8D",
    "colab_type": "text"
   },
   "source": [
    "Now, we will handle the video stream from your webcam through JavaScript. Running the following two cells will\n",
    "define the functions necessary to establish the stream."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SbrDWMxkmW8D",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "'''\n",
    "ADAPTED FROM https://emadehsan.com/p/object-detection,\n",
    "https://colab.research.google.com/drive/1HPrxuPjJDvEx64TlY7K8SPK_XbOJTZ7A\n",
    "## Camera Capture\n",
    "Using a webcam to capture images for processing on the runtime.\n",
    "Source: https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi\n",
    "'''\n",
    "\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.5):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'none';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      // show the video in the HTML element\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      // google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // prints the logs to cell\n",
    "      let jsLog = function(abc) {\n",
    "        document.querySelector(\"#output-area\").appendChild(document.createTextNode(`${abc}... `));\n",
    "      }\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      // await new Promise((resolve) => capture.onclick = resolve);\n",
    "        const canvas = document.createElement('canvas');\n",
    "        canvas.width = video.videoWidth;\n",
    "        canvas.height = video.videoHeight;\n",
    "\n",
    "      for (let i = 0; i < 1; i++) {\n",
    "        canvas.style.display = \"none\";\n",
    "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "        img = canvas.toDataURL('image/jpeg', quality);\n",
    "\n",
    "        // show each captured image\n",
    "        // let imgTag = document.createElement('img');\n",
    "        // imgTag.src = img;\n",
    "        // div.appendChild(imgTag);\n",
    "\n",
    "        // jsLog(i + \"sending\")\n",
    "        // Call a python function and send this image\n",
    "        // google.colab.kernel.invokeFunction('notebook.get_img', [img], {});\n",
    "        // jsLog(i + \"SENT\")\n",
    "\n",
    "        // wait for X miliseconds second, before next capture\n",
    "        // await new Promise(resolve => setTimeout(resolve, 10));\n",
    "      }\n",
    "      stream.getVideoTracks()[0].stop(); // stop video stream\n",
    "      return(img)\n",
    "    }\n",
    "    ''')\n",
    "  display(js) # make the provided HTML, part of the cell\n",
    "  data = eval_js('takePhoto({})'.format(quality)) # call the takePhoto() JavaScript function\n",
    "  output_frame = get_img(data)\n",
    "  return output_frame "
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "crW4n1c6mW8F",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def data_uri_to_img(uri):\n",
    "  \"\"\"convert base64image to numpy array\"\"\"\n",
    "  try:\n",
    "    image = base64.b64decode(uri.split(',')[1], validate=True)\n",
    "    # make the binary image, a PIL image\n",
    "    image = Image.open(BytesIO(image))\n",
    "    # convert to numpy array\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    return image\n",
    "  except Exception as e:\n",
    "    logging.exception(e);print('\\n')\n",
    "    return None\n",
    "\n",
    "def get_img(img):\n",
    "    frame = data_uri_to_img(img)\n",
    "    frame = skimage.color.gray2rgb(frame)\n",
    "    return frame"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "02SRcGcPmW8I",
    "colab_type": "text"
   },
   "source": [
    "At last, we are ready to run the script!\n",
    "\n",
    "NOTE: Currently this cloud-based implementation runs very slowly (<1 FPS)\n",
    "and is intended as a demonstration of the code's structure only. For best results, follow the instructions [on our GitHub](https://github.com/bf777/DeepCut2RealTime) to run our software locally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wv-gU7ix72RV",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Change batch size to 1\n",
    "edits = {'batch_size': 1}\n",
    "if modelzoo:\n",
    "  auxiliaryfunctions.edit_config(config_path[0], edits)\n",
    "else:\n",
    "  auxiliaryfunctions.edit_config(config_path, edits)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qQicxzwIFj0w",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Output folder (for saving data and images)\n",
    "destfolder = os.path.join('/content/drive/My Drive', gDriveParentFolder)"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Dr7a6IX0mW8I",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "'''\n",
    "Run the script itself!\n",
    "NOTE: only set save_frames=True if you have sufficient space in your google\n",
    "drive!! This option saves ALL frames that are processed if set to True.\n",
    "'''\n",
    "if modelzoo:\n",
    "  analyze_stream(config_path[0], destfolder, y_left, y_right, y_upper_lim, \n",
    "               refractory_time, save_frames=False, save_as_csv=True)\n",
    "else:\n",
    "  analyze_stream(config_path, destfolder, y_left, y_right, y_upper_lim, \n",
    "               refractory_time, save_frames=False, save_as_csv=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QurrtygQMUws",
    "colab_type": "text"
   },
   "source": [
    "# Docstring for analyze_stream:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPpWI7kbHnmM",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Docstring for analyze_stream:\n",
    "\"\"\"\n",
    "def analyze_stream(config, destfolder, shuffle=1, trainingsetindex=0, gputouse=0, save_as_csv=False, save_frames=True,\n",
    "                   cropping=None, baseline=True, name=\"default_animal\", camtype=\"colab\"):\n",
    "Makes prediction based on a trained network. The index of the trained network is specified by parameters in the config file (in particular the variable 'snapshotindex')\n",
    "\n",
    "You can crop the video (before analysis), by changing 'cropping'=True and setting 'x1','x2','y1','y2' in the config file. The same cropping parameters will then be used for creating the video.\n",
    "\n",
    "Output: The labels are stored as MultiIndex Pandas Array, which contains the name of the network, body part name, (x, y) label position \\n\n",
    "        in pixels, and the likelihood for each frame per body part. These arrays are stored in an efficient Hierarchical Data Format (HDF) \\n\n",
    "        in the same directory, where the video is stored. However, if the flag save_as_csv is set to True, the data can also be exported in \\n\n",
    "        comma-separated values format (.csv), which in turn can be imported in many programs, such as MATLAB, R, Prism, etc.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "config : string\n",
    "    Full path of the config.yaml file as a string.\n",
    "destfolder : string\n",
    "    Full path of the directory to which you want to output data and (optionally) saved frames.\n",
    "\n",
    "shuffle: int, optional\n",
    "    An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.\n",
    "trainingsetindex: int, optional\n",
    "    Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).\n",
    "\n",
    "gputouse: int, optional. Natural number indicating the number of your GPU (see number in nvidia-smi). If you do not have a GPU put None.\n",
    "See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n",
    "save_as_csv: bool, optional\n",
    "    Saves the predictions in a .csv file. The default is ``False``; if provided it must be either ``True`` or ``False``\n",
    "save_frames: bool, optional\n",
    "    Labels and saves each frame of the stream to the destfolder defined above. The default is ``False``; if provided it must be either ``True`` or ``False``\n",
    "cropping: bool, optional\n",
    "    Selects whether to apply cropping to each frame or not. Not recommended as it increases computational overhead.\n",
    "baseline: bool, optional\n",
    "    Selects whether the current trial is a baseline trial (movement tracking but no reinforcement) or a training trial (movement tracking with reinforcement\n",
    "    via water reward). If True, current trial is baseline trial; else (e.g. False), current trial is training trial.\n",
    "name: string, optional\n",
    "    Pass in the name/subject ID of the animal to be observed in the current trial. This will ensure that the animal is named consistently in data output and\n",
    "    trial metadata.\n",
    "camtype: string, optional\n",
    "    Pass in the camera configuration to use. If using a sentech camera, use `sentech`. If using a standard OpenCV webcam\n",
    "    when running the script on your local machine (not Colab!), use `opencv`. In Colab, use `colab` (will access webcam via Javascript).\n",
    "Examples\n",
    "--------\n",
    "If you want to analyze a stream without saving anything\n",
    ">>> deeplabcut.analyze_stream('/analysis/project/reaching-task/config.yaml','/analysis/project/reaching-task/output')\n",
    "--------\n",
    "If you want to analyze a stream and save just the labelled frames\n",
    ">>> deeplabcut.analyze_stream('/analysis/project/reaching-task/config.yaml','/analysis/project/reaching-task/output', save_frames=True)\n",
    "--------\n",
    "If you want to analyze a stream and save both the frames and a .csv file with the coordinates of the labels\n",
    ">>> deeplabcut.analyze_stream('/analysis/project/reaching-task/config.yaml','/analysis/project/reaching-task/output', save_as_csv=True, save_frames=True)\n",
    "--------\n",
    "\"\"\"\n",
    "```\n",
    "\n"
   ]
  }
 ]
}